{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1667fe3b-e934-49ab-a2c4-28266b5ed71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365a8bdc-74eb-441b-a980-67eab1400e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b62b721-6c0e-4d12-825e-7c89ac576c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes input distance array and flattens feature maps\n",
    "def get_encoder():\n",
    "    inputs = tf.keras.Input(shape = (100,100,1))\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=6, strides=(3, 3), activation='relu')(inputs)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=6, strides=(3, 3), activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    return tf.keras.Model(inputs=inputs,outputs=[x])\n",
    "\n",
    "# gets flattened feature maps, and one hot label vector and outputs mu and signma\n",
    "def get_conditional_encoder(latent_dim,input_size):\n",
    "    inputs = tf.keras.Input(shape = (input_size + 3,))\n",
    "    mu = tf.keras.layers.Dense(units=latent_dim)(inputs)\n",
    "    sigma = tf.keras.layers.Dense(units=latent_dim)(inputs)\n",
    "    return  tf.keras.Model(inputs=inputs,outputs=[mu,sigma])\n",
    "\n",
    "# get vae decoder that takes as input the latent representation and the label \n",
    "# for the data (ion type and ions concentration)\n",
    "def get_conditional_decoder(latent_dim):\n",
    "    z = tf.keras.Input(shape = (latent_dim+3,))\n",
    "    x= tf.keras.layers.Dense(units=25*25*32, activation='relu')(z)\n",
    "    x=tf.keras.layers.Reshape(target_shape=(25, 25, 32))(x)\n",
    "    x=tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=6, strides=2, padding='same',activation='relu')(x)\n",
    "    x=tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=6, strides=2, padding='same',activation='relu')(x)\n",
    "    decoded_img=tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=z,outputs=[decoded_img])\n",
    "\n",
    "class Conditional_VAE(tf.keras.Model):\n",
    "    def __init__(self,latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = get_encoder()\n",
    "        # 5184 is specific to input size, use function to get encoder info instead\n",
    "        self.conditional_encoder = get_conditional_encoder(latent_dim=latent_dim,input_size=5184)\n",
    "        self.decoder_block = get_conditional_decoder(latent_dim)\n",
    "\n",
    "    def call(self,img,labels):\n",
    "        # encoder q(z|x,y)\n",
    "        enc1_output = self.encoder(img)\n",
    "        # concat feature maps and label (onehot vector for Na or Mg and numerical value for concentration)\n",
    "        img_lbl_concat = np.concatenate((enc1_output,labels),axis=1)\n",
    "        z_mu,z_sigma = self.conditional_encoder(img_lbl_concat)\n",
    "\n",
    "        # sampling\n",
    "        epsilon = tf.random.normal(shape=z_mu.shape,mean=0.0,stddev=1.0)\n",
    "        z = z_mu + tf.math.softplus(z_sigma) * epsilon\n",
    "\n",
    "        # decoder p(x|z,y)\n",
    "        z_lbl_concat = np.concatenate((z,labels),axis=1)\n",
    "        decoded_img = self.decoder_block(z_lbl_concat)\n",
    "\n",
    "        return z_mu,z_sigma,decoded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55400532-a20b-43cb-9c1a-d54391a41774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        1184      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          73792     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 5184)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,976\n",
      "Trainable params: 74,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 5187)]       0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 30)           155640      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 30)           155640      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 311,280\n",
      "Trainable params: 311,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 33)]              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20000)             680000    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 25, 25, 32)        0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 50, 50, 64)       73792     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 100, 100, 32)     73760     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 100, 100, 1)      289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 827,841\n",
      "Trainable params: 827,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(get_encoder().summary())\n",
    "print(get_conditional_encoder(latent_dim=30,input_size=5184).summary())\n",
    "print(get_conditional_decoder(latent_dim=30).summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f72dde-a7b8-4846-ad6e-ac2a8283a5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2b91bff22880>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e95c6c9b-0466-43f2-8dee-1084d160d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(z_mu,z_sigma):\n",
    "    sigma_squared = tf.math.softplus(z_sigma) ** 2\n",
    "    kl_1d = -0.5 * (1 + tf.math.log(sigma_squared) - z_mu ** 2 - sigma_squared)\n",
    "\n",
    "    # sum over sample dim, average over batch dim\n",
    "    kl_batch = tf.reduce_mean(tf.reduce_sum(kl_1d,axis=1))\n",
    "\n",
    "    return kl_batch\n",
    "\n",
    "def elbo(z_mu,z_sigma,decoded_img,original_img):\n",
    "    # reconstruction loss\n",
    "    mse = tf.reduce_mean(tf.reduce_sum(tf.square(original_img - decoded_img),axis=1))\n",
    "    # kl loss\n",
    "    kl = kl_loss(z_mu,z_sigma)\n",
    "\n",
    "    return mse,kl\n",
    "\n",
    "def train(latent_dim,beta,epochs,train_ds, dataset_mean, dataset_std):\n",
    "\n",
    "    model = Conditional_VAE(latent_dim)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "    kl_loss_tracker = tf.keras.metrics.Mean(name='kl_loss')\n",
    "    mse_loss_tracker = tf.keras.metrics.Mean(name='mse_loss')\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        label_list = None\n",
    "        z_mu_list = None    \n",
    "\n",
    "        for _,(imgs,labels) in train_ds.enumerate():\n",
    "            \n",
    "            # training loop\n",
    "            with tf.GradientTape() as tape:\n",
    "                # forward pass\n",
    "                z_mu,z_sigma,decoded_imgs = model(imgs,labels)\n",
    "\n",
    "                # compute loss\n",
    "                mse,kl = elbo(z_mu,z_sigma,decoded_imgs,imgs)\n",
    "                loss = mse + beta * kl\n",
    "            \n",
    "            # compute gradients\n",
    "            gradients = tape.gradient(loss,model.variables)\n",
    "\n",
    "            # update weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "\n",
    "            # update metrics\n",
    "            kl_loss_tracker.update_state(beta * kl)\n",
    "            mse_loss_tracker.update_state(mse)\n",
    "\n",
    "            # save encoded means and labels for latent space visualization\n",
    "            if label_list is None:\n",
    "                label_list = labels\n",
    "            else:\n",
    "                label_list = np.concatenate((label_list,labels))\n",
    "                \n",
    "            if z_mu_list is None:\n",
    "                z_mu_list = z_mu\n",
    "            else:\n",
    "                z_mu_list = np.concatenate((z_mu_list,z_mu),axis=0)\n",
    "                \n",
    "        # generate new samples\n",
    "        generate_conditioned_distance_array(model,dataset_mean,dataset_std, epoch)\n",
    "\n",
    "        # display metrics at the end of each epoch.\n",
    "        epoch_kl,epoch_mse = kl_loss_tracker.result(),mse_loss_tracker.result()\n",
    "        print(f'epoch: {epoch}, mse: {epoch_mse:.4f}, kl_div: {epoch_kl:.4f}')\n",
    "\n",
    "        # reset metric states\n",
    "        kl_loss_tracker.reset_state()\n",
    "        mse_loss_tracker.reset_state()\n",
    "\n",
    "    return model,z_mu_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502ab829-cea2-44d4-9ab5-20eec3bb4b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 46\n",
      "test: 6\n"
     ]
    }
   ],
   "source": [
    "def _flatten(sample):\n",
    "    array = tf.reshape(sample,[-1])\n",
    "    return array\n",
    "\n",
    "def prepare_data(dataset_name):\n",
    "    train_dataset = tf.data.Dataset.load(\"dataset/m1_ssRNA_train_\" + dataset_name)\n",
    "    test_dataset = tf.data.Dataset.load(\"dataset/m1_ssRNA_test_\" + dataset_name)\n",
    "    input_shape = train_dataset.element_spec[0].shape\n",
    "    train_dataset = (train_dataset\n",
    "                     .shuffle(int(10e3))\n",
    "                     # .map(lambda data, label: (_flatten(data), label))\n",
    "                     .batch(20)\n",
    "                     .prefetch(tf.data.AUTOTUNE))\n",
    "    test_dataset = (test_dataset\n",
    "                    .shuffle(int(10e3))\n",
    "                    # .map(lambda data, label: (_flatten(data), label))\n",
    "                    .batch(20)\n",
    "                    .prefetch(tf.data.AUTOTUNE))\n",
    "    # check size of dataset\n",
    "    print(f'train: {train_dataset.cardinality().numpy()}')\n",
    "    print(f'test: {test_dataset.cardinality().numpy()}')\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "dataset_name = \"onehot_ion_concat_conc\"\n",
    "train_ds, test_ds = prepare_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4afd8027-6cf1-44f4-820b-b5f418e7d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained model to predict the distance array for RNA molcule in solution with 50mM per L Na+\n",
    "def generate_conditioned_distance_array(model,dataset_mean,dataset_std, epoch):\n",
    "    n = 20  # number of generation per distance array\n",
    "    distance_df = np.zeros((n, 100, 100, 1))\n",
    "    for gen_idx in range(n):\n",
    "        # currently testing out Na+ and 50 mM conc, double bracket to conform to shape (1,...)\n",
    "        label = np.array([[1,0,50]])\n",
    "\n",
    "        z = tf.random.normal(shape=(1,model.conditional_encoder.output[0].shape[1]),mean=0.0,stddev=1.0)\n",
    "        z_lbl_concat = np.concatenate((z,label),axis=1)\n",
    "        preds = model.decoder_block(z_lbl_concat)\n",
    "\n",
    "        generated_df = tf.reshape(preds[0],[-1,100,100,1])\n",
    "        generated_df = (generated_df * dataset_std) + dataset_mean\n",
    "        distance_df[gen_idx] = generated_df.numpy()\n",
    "    np.savez(f'results/{epoch}_Na+_50_predicted_df.npz', distance_df)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba2f8334-298d-40ce-a6be-7c8ec319aef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, mse: 32.7124, kl_div: 1.9374\n",
      "epoch: 1, mse: 13.0811, kl_div: 0.0851\n",
      "epoch: 2, mse: 7.2414, kl_div: 0.0172\n",
      "epoch: 3, mse: 5.5444, kl_div: 0.0096\n",
      "epoch: 4, mse: 5.1890, kl_div: 0.0070\n",
      "epoch: 5, mse: 4.9541, kl_div: 0.0055\n",
      "epoch: 6, mse: 4.7302, kl_div: 0.0045\n",
      "epoch: 7, mse: 4.6520, kl_div: 0.0037\n",
      "epoch: 8, mse: 4.6034, kl_div: 0.0031\n",
      "epoch: 9, mse: 4.4814, kl_div: 0.0026\n"
     ]
    }
   ],
   "source": [
    "beta = 1\n",
    "epochs = 10\n",
    "latent_dim = 30\n",
    "\n",
    "model,z_mu_list,label_list = train(latent_dim,beta,epochs,train_ds, data_mean, data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1610a146-9617-4e79-9b95-eb2e02ad764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func must be function that takes in model, ion, and conc as parameters in this order\n",
    "def apply_to_dict(func):\n",
    "    return {model: {ion: {conc: func(model, ion, conc)\n",
    "                           for conc in CONCS} \n",
    "                     for ion in IONS}\n",
    "             for model in MODEL_N}\n",
    "MODEL_N = [i for i in range(1, 2)]\n",
    "IONS = [\"Na+\", \"MG\"]\n",
    "CONCS = [c for c in range(10, 60, 10)]\n",
    "TIME_FRAMES = [t for t in range(101)]\n",
    "datafiles = apply_to_dict(lambda m, i, c: f\"../../data/distance_npz/filtered_sasdfb9_m{m}_{c}_{i}.npz\")\n",
    "npz_dict = apply_to_dict(lambda m, i, c: np.load(datafiles[m][i][c])[\"arr_0\"])\n",
    "\n",
    "spec_tuple = [(m,i,c,t) for m in MODEL_N for i in IONS for c in CONCS for t in TIME_FRAMES]\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(spec_tuple)\n",
    "# construct dataset with label shuffled\n",
    "label = np.array([[int(i==\"Na+\"), int(i==\"MG\"), c] for m,i,c,t in spec_tuple])\n",
    "data = np.stack([npz_dict[m][i][c][t] for m, i, c, t in spec_tuple])\n",
    "\n",
    "# standardization\n",
    "data = data/np.max(data)\n",
    "data_mean = np.mean(data)\n",
    "data_std = np.std(data)\n",
    "data = (data - data_mean) / (data_std)\n",
    "\n",
    "# split set into train and test\n",
    "n_set = len(label)\n",
    "# split train and test dataset to be 90% and 10% of the entire dataset\n",
    "split_idx = math.ceil(n_set * 0.9)\n",
    "# construct train dataset\n",
    "train_feature_dataset = tf.data.Dataset.from_tensor_slices(data[:split_idx])\n",
    "train_label_dataset = tf.data.Dataset.from_tensor_slices(label[:split_idx])\n",
    "train_dataset = tf.data.Dataset.zip((train_feature_dataset, train_label_dataset))\n",
    "# construct test dataset\n",
    "test_feature_dataset = tf.data.Dataset.from_tensor_slices(data[split_idx:])\n",
    "test_label_dataset = tf.data.Dataset.from_tensor_slices(label[split_idx:])\n",
    "test_dataset = tf.data.Dataset.zip((test_feature_dataset, test_label_dataset))\n",
    "\n",
    "# save dataset\n",
    "# dataset_name = \"onehot_ion_concat_conc\"\n",
    "# train_dataset.save(f\"dataset/CVAE_m1_ssRNA_train_{dataset_name}\")\n",
    "# test_dataset.save(f\"dataset/CVAE_m1_ssRNA_test_{dataset_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
